{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwVCG9w_wvtF"
      },
      "outputs": [],
      "source": [
        "# Milestone 5\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load cleaned dataset\n",
        "df = pd.read_csv('../data/liver_dataset_cleaned.csv')\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdZIgMwrw4Oc",
        "outputId": "e0bf4d7e-6c79-426d-dae3-d5b6f6475d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Base Model Performance\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.24      0.34        33\n",
            "           1       0.75      0.93      0.83        83\n",
            "\n",
            "    accuracy                           0.73       116\n",
            "   macro avg       0.66      0.59      0.59       116\n",
            "weighted avg       0.70      0.73      0.69       116\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train default Random Forest\n",
        "base_model = RandomForestClassifier(random_state=42)\n",
        "base_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_base = base_model.predict(X_test)\n",
        "\n",
        "print(\"üìå Base Model Performance\")\n",
        "print(classification_report(y_test, y_pred_base))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3J0_pziw5b0",
        "outputId": "cdd5a3e5-9001-46fb-95b6-88a4f0360d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
            "‚úÖ Best Hyperparameters: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "                    param_grid=param_grid,\n",
        "                    cv=3,\n",
        "                    n_jobs=-1,\n",
        "                    verbose=1,\n",
        "                    scoring='f1')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"‚úÖ Best Hyperparameters:\", grid.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oIC8ws4w8Dp",
        "outputId": "8ba58d91-4422-42c7-9b08-75fce5ef52a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Tuned Model Performance\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.03      0.06        33\n",
            "           1       0.72      0.99      0.83        83\n",
            "\n",
            "    accuracy                           0.72       116\n",
            "   macro avg       0.61      0.51      0.44       116\n",
            "weighted avg       0.66      0.72      0.61       116\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict using best estimator\n",
        "best_model = grid.best_estimator_\n",
        "y_pred_tuned = best_model.predict(X_test)\n",
        "\n",
        "print(\"üìå Tuned Model Performance\")\n",
        "print(classification_report(y_test, y_pred_tuned))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u38ZS4IxIO_",
        "outputId": "602c38da-6c7e-415e-d9f7-a31945f34a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Comparison for accuracy:\n",
            "Base: 0.7328 | Tuned: 0.7155\n",
            "\n",
            "üîç Comparison for precision:\n",
            "Base: 0.7549 | Tuned: 0.7193\n",
            "\n",
            "üîç Comparison for recall:\n",
            "Base: 0.9277 | Tuned: 0.9880\n",
            "\n",
            "üîç Comparison for f1:\n",
            "Base: 0.8324 | Tuned: 0.8325\n"
          ]
        }
      ],
      "source": [
        "# Compare metrics manually\n",
        "def print_comparison(name, y_true, y_pred_base, y_pred_tuned):\n",
        "    print(f\"\\nüîç Comparison for {name}:\")\n",
        "    base = eval(name + '_score')(y_true, y_pred_base)\n",
        "    tuned = eval(name + '_score')(y_true, y_pred_tuned)\n",
        "    print(f\"Base: {base:.4f} | Tuned: {tuned:.4f}\")\n",
        "\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    print_comparison(metric, y_test, y_pred_base, y_pred_tuned)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
